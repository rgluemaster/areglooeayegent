\documentclass[11pt]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[section]{placeins}
\usepackage{abstract}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont} 
\numberwithin{equation}{section}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\begin{document}

\title{DMUU Project Report}
\author{\begin{tabular}{cc}
Roland Hellström & Sebastian Ånerud \\
XXXXXX-xxxx & 910407-5958 \\
asdf.comasd@ & anerud@student.chalmers.se
\end{tabular}}
\date{\today}
\maketitle

\begin{center}
\centering
\includegraphics[scale=1]{DOGE}
\end{center}

\newpage 

\begin{flushleft}

\section{Introduction}

This project consists of building an agent that is able to act and take optimal decisions in unknown and arbitrary environments. The only thing known about the environments, that the agent is supposed to act in, is the number of states, the number of actions and that the underlying model is a Markov Decision Process. Also it is known that one of the environments is a Partially Observable MDP (POMDP). However, the agent presented in this report does not implement any algorithm specifically designed to act well in such an environment. The algorithms used by the agent are a Generalized Stochastic Value Iteration- and an Upper Confidence Bound-Algorithm. Some of the environments that the agent are tested on are a simple 2-arm bandit-, n-armed bandit-, mines-, tic tac toe- and chain-environment.

\section{Method}

In this section the choice of algorithms and the motivation behind choice will be discussed briefly. However, the algorithms themselves will not be explained thoroughly and it is assumed that the reader either have knowledge about the algorithms or takes the time to understand them before reading any further. Also the environments used for testing and verifying the agent will be discussed.  \newline

\subsection{Testing environments}

\subsubsection{Bandit environments}

The bandit environment is a simple environment with only one state and $n$ actions. This environment is used to test the Upper Confidence Bound algorithm.
\subsubsection{Mines environment}
The mines environment is a grid environment where a few states gives a huge negative reward and only one state gives a positive reward. Moving in the environment gives a reward of -1. This environment is used to test that the agent does not make the same mistake of stepping on a mine too many times.

\subsubsection{Chain environment}

In the mines environment there were only one positive reward and therefore no risk of getting stuck in a sub-optimal maximum. The chain environment consists of only a few states where the starting state gives a positive but small reward and another state, which is harder to reach, gives a larger positive reward. This environment test the agents ability to explore the environment and not getting stuck in a sub-local maxima (the starting point).
\subsubsection{Tic tac toe environment}

The tic tac toe environment is as it sounds the game of tic tac toe and consists of $3^9=19683$ states and 9 actions. The actions corresponds to placing an X on a tile and trying to place it on a tile where there already is an X or O results in -10 reward. Winning the game gives +10 reward and losing the game gives -1 reward. This environment has a large state space compared to the other environments and is used to test the agents ability to act in an environment where most of the states only appears once or not even at all.
\subsection{Algorithms used by agent}

\subsubsection{Upper Confidence Bound}

The Upper Confidence Bound algorithm is used by our agent when the environment consists only of one state (a bandit environment). The algorithm uses a heuristic for choosing the arm that it thinks is the best. The heuristic used by the agent is:

$$h(a) = r(a) + \sqrt{\frac{2ln(t)}{n_a}},$$

where $r(a)$ is the mean of rewards observed by taking action $a$, $n_a$ is the number of times action $a$ have been taken and $t$ is the number of times the bandit have been played. In the sample for $r(a)$ a fake observation with $r_{max}$ was added to keep an optimistic heuristic which favours exploration. The agent chooses the action with the greatest heuristic at all times.

\subsubsection{Generalized Stochastic Value iteration}

If the environment consists of two states or more, the agent uses an $\epsilon$-greedy Generalized Stochastic Value Iteration algorithm with an estimated MDP. The MDP is estimated by counting the number of transitions from one state to another given that an action was taken (Dirichlet model). From beginning it is assumed that it is not possible to reach any state from any other state (count starts at 0 for all transitions). The algorithm chooses the seemingly best action with probability $1-\epsilon$ and a random action with probability $\epsilon$. The whole algorithm the agent uses can be found at http://www.cse.chalmers.se/\textasciitilde chrdimi/teaching/optimal\textunderscore decisions/reinforcement\textunderscore learning\textunderscore article.pdf.

\section{Results}

Present the results from the different environments.

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the mines environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & -5509.8 \\ \hline
$n_s^{-1}$ & -2912.3 \\ \hline
$n_s^{-2}$ & -1603.6 \\ \hline
$_s^{-5}$ & -1384.0 \\
\hline
\end{tabular}
\label{tab:minesMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the chain environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & ANS \\ \hline
$n_s^{-1}$ & ANS \\ \hline
$n_s^{-2}$ & ANS \\
\hline
\end{tabular}
\label{tab:chainMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the tic tac toe environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & ANS \\ \hline
$n_s^{-1}$ & ANS \\ \hline
$n_s^{-2}$ & ANS \\
\hline
\end{tabular}
\label{tab:tttMean}
\end{center}
\end{table}

\subsection{Parameter choices}

Results showed that having a high $\epsilon$ was advantageous in environments like the chain environment while in the mines environment a high $\epsilon$ was causing the agent to step on mines even though it had made that mistake several times before. In the tic tac toe environment it showed to be advantageous to have a low exploration rate. Since the results pointed out that in most environments it was beneficial to have a low, but still existing, exploration rate. A trade-off between exploration and exploitation was found when $\epsilon = \frac{1}{n_s}$, where $n_s$ is the number of visits in state $s$. \newline

There were no other parameter tried for the UCB algorithm other than the ones stated in the method section. The lecture notes does a great job arguing for why the choice of parameters is good.

\section{Discussion}

Discuss the results. bla bla voff voff.

\section{Conclusion}

very much conclude. so amaze. wow.

\end{flushleft}

\end{document}