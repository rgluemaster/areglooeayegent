\documentclass[11pt]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[section]{placeins}
\usepackage{abstract}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont} 
\numberwithin{equation}{section}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\begin{document}

\title{DMUU Project Report}
\author{\begin{tabular}{cc}
Roland Hellström & Sebastian Ånerud \\
XXXXXX-xxxx & 910407-5958 \\
asdf.comasd@ & anerud@student.chalmers.se
\end{tabular}}
\date{\today}
\maketitle

\begin{center}
\centering
\includegraphics[scale=1]{DOGE}
\end{center}

\newpage 

\begin{flushleft}

\section{Introduction}

This project consists of building an agent that is able to act and take optimal decisions in unknown and arbitrary environments. The only thing known about the environments, that the agent is supposed to act in, is the number of states, the number of actions and that the underlying model is a Markov Decision Process. Also it is known that one of the environments is a Partially Observable MDP (POMDP). However, the agent presented in this report does not implement any algorithm specifically designed to act well in such an environment. The algorithms used by the agent are a Generalized Stochastic Value Iteration- and an Upper Confidence Bound-Algorithm. Some of the environments that the agent are tested on are a simple 2-arm bandit-, n-armed bandit-, mines-, tic tac toe- and chain-environment.

\section{Method}

In this section the choice of algorithms and the motivation behind choice will be discussed briefly. However, the algorithms themselves will not be explained thoroughly and it is assumed that the reader either have knowledge about the algorithms or takes the time to understand them before reading any further. Also the environments used for testing and verifying the agent will be discussed.  \newline

\subsection{Testing environments}

\subsubsection{Bandit environments}
\subsubsection{Mines environment}
\subsubsection{Chain environment}
\subsubsection{Tic tac toe environment}

\subsection{Algorithms used by agent}

\subsubsection{Upper Confidence Bound}

The Upper Confidence Bound algorithm is used by our agent when the environment consists only of one state (a bandit environment). The algorithm uses a heuristic for choosing the arm that it thinks is the best. The heuristic used by the agent is:

$$h(a) = r(a) + \sqrt{\frac{2ln(t)}{n_a}},$$

where $r(a)$ is the mean of rewards observed by taking action $a$, $n_a$ is the number of times action $a$ have been taken and $t$ is the number of times the bandit have been played. In the sample for $r(a)$ a fake observation with $r_{max}$ was added to keep an optimistic heuristic which favours exploration. The agent chooses the action with the greatest heuristic at all times.

\subsubsection{Generalized Stochastic Value iteration}

If the environment consists of two states or more, the agent uses an $\epsilon$-greedy Generalized Stochastic Value Iteration algorithm with an estimated MDP. The MDP is estimated by counting the number of transitions from one state to another given that an action was taken (Dirichlet model). The algorithm chooses the seemingly best action with probability $1-\epsilon$ and a random action with probability $\epsilon$. The whole algorithm the agent uses can be found at http://www.cse.chalmers.se/\textasciitilde chrdimi/teaching/optimal\textunderscore decisions/reinforcement\textunderscore learning\textunderscore article.pdf.

\section{Results}

Present the results from the different environments.

\subsection{Parameter choices}

$\epsilon$ is defined to be $\frac{1}{n_s}$, where $n_s$ is the number of visits in state $s$.

\section{Discussion}

Discuss the results. bla bla voff voff.

\section{Conclusion}

very much conclude. so amaze. wow.

\end{flushleft}

\end{document}