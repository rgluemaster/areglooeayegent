\documentclass[11pt]{article}

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[section]{placeins}
\usepackage{abstract}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont} 
\numberwithin{equation}{section}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\begin{document}

\title{DMUU Project Report}
\author{\begin{tabular}{cc}
Roland Hellström Keyte & Sebastian Ånerud \\
880728-1194 & 910407-5958 \\
rolandh@student.chalmers.se & anerud@student.chalmers.se
\end{tabular}}
\date{\today}
\maketitle

\newpage 

\begin{flushleft}

\section{Introduction}

This project consists of building an agent that is able to act and take optimal decisions in unknown and arbitrary environments. The only thing known about the environments, that the agent is supposed to act in, is the number of states, the number of actions and that the underlying model is a Markov Decision Process. Also it is known that one of the environments is a Partially Observable MDP (POMDP). However, the agent presented in this report does not implement any algorithm specifically designed to act well in such an environment. The algorithms used by the agent are a Generalized Stochastic Value Iteration- and an Upper Confidence Bound-Algorithm. Some of the environments that the agent are tested on are a simple 2-arm bandit-, n-armed bandit-, mines-, tic tac toe- and chain-environment.

\section{Testing Environments}

Here different environments, and the properties they are supposed to test in the agent, are presented.

\subsection{Bandit environments}

The bandit environment is a simple environment with only one state and $n$ actions. This environment is used to test the Upper Confidence Bound algorithm.
\subsection{Mines environment}
The mines environment is a grid environment where a few states gives a huge negative reward and only one state gives a positive reward. Moving in the environment gives a reward of -1. This environment is used to test that the agent does not make the same mistake of stepping on a mine too many times.

\subsection{Chain environment}

In the mines environment there were only one positive reward and therefore no risk of getting stuck in a sub-optimal maximum. The chain environment consists of only a few states where the starting state gives a positive but small reward and another state, which is harder to reach, gives a larger positive reward. This environment test the agents ability to explore the environment and not getting stuck in a sub-local maxima (the starting point).

\subsection{Tic tac toe environment}

The tic tac toe environment is as it sounds the game of tic tac toe and consists of $3^9=19683$ states and 9 actions. The actions corresponds to placing an X on a tile and trying to place it on a tile where there already is an X or O results in -10 reward. Winning the game gives +10 reward and losing the game gives -1 reward. This environment has a large state space compared to the other environments and is used to test the agents ability to act in an environment where most of the states only appears once or not even at all.

\section{The Agent}

In this section the choice of algorithms and the motivation behind choice will be discussed briefly. However, the algorithms themselves will not be explained thoroughly and it is assumed that the reader either have knowledge about the algorithms or takes the time to understand them before reading any further. There will also be a short section explaining some of the thoughts behind the implementation of the MDP model. \newline

The agent is equipped with essentially two different algorithms, the Upper Confidence Bound and the Generalized Stochastic Value Iteration algorithm which uses a Dirichlet as the underlying model of the MDP. The choice between the algorithms is really simple: If the number of states equals 1 the agent uses the UCB-algorithm and otherwise it uses the GSVI-algorithm.

\subsection{Upper Confidence Bound}

The Upper Confidence Bound algorithm is used by our agent when the environment consists only of one state (a bandit environment). The algorithm uses a heuristic for choosing the arm that it thinks is the best. The heuristic used by the agent is:

$$h(a) = r(a) + \sqrt{\frac{2ln(t)}{n_a}},$$

where $r(a)$ is the mean of rewards observed by taking action $a$, $n_a$ is the number of times action $a$ have been taken and $t$ is the number of times the bandit have been played. In the sample for $r(a)$ a fake observation with $r_{max}$ was added to keep an optimistic heuristic which favours exploration. The agent chooses the action with the greatest heuristic at all times.

\subsection{Generalized Stochastic Value iteration}

If the environment consists of two states or more, the agent uses an $\epsilon$-greedy Generalized Stochastic Value Iteration algorithm with an estimated MDP. The MDP is estimated by counting the number of transitions from one state to another given that an action was taken (Dirichlet model). From beginning it is assumed that it is not possible to reach any state from any other state (count starts at 0 for all transitions). The algorithm chooses the seemingly best action with probability $1-\epsilon$ and a random action with probability $\epsilon$. The whole algorithm the agent uses can be found at http://www.cse.chalmers.se/\textasciitilde chrdimi/teaching/optimal\textunderscore decisions/reinforcement\textunderscore learning\textunderscore article.pdf.

\subsection{The implementation of the MDP model}

Since there is no way of knowing a priori how many states an environment will contain, the agent cannot simply initialize all the arrays from the beginning. This would cause a heap size overflow if the state space is large as in the tic tac toe environment. This was empirically confirmed during the implementation phase of this agent. Instead, the agent uses a HashMap-data structure where it only puts the states which it has encountered. In this way the agent will only keep track of the fraction of the states space which it has visited. With this implementation the agent is allowed to run the GSVI-algorithm on almost any environment.  

\subsection{Large state-spaces}

Experiments were conducted to test some simple heuristics for large state-spaces. This was done in order to efficiently deal with environments with large state-spaces such as the tic-tac-toe environment. However, none of the environments on which the agent was tested had large enough state-spaces for the computation time to become significantly long. Therefore, only a few online-algorithms utilizing feature-spaces were initially tested. \newline

The feature spaces were created by the agent as the state-space was explored. Each new state was initially aggregated depending only on the action, or sequence of two actions, which brought the agent there. When the number of visits to a certain non-aggregated state reached a certain threshold, the aggregated counts for that state was separated to an own non-aggregated state. \newline

The idea behind the action-heuristic was that it, as well as reducing the size of the state-space, could potentially provide a good way of exploring earlier unvisited states or states with few visits in a better than uniformly random way. The intuition behind the idea was that similar actions would often bring the agent to similar states.

\section{Results}

Here results from running the agent on different environments with different parameters are shown. The parameter $\epsilon$ takes on the values $n^{-0.5},n^{-1},n^{-2}$ and $n^{-5}$ in the mines-, chain-, loop-, and tic tac toe environment. Also results for the UCB-algorithm on a 10-armed bandit environment is shown.

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the mines environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & -5509.8 \\ \hline
$n_s^{-1}$ & -2912.3 \\ \hline
$n_s^{-2}$ & -1603.6 \\ \hline
$_s^{-5}$ & -1384.0 \\
\hline
\end{tabular}
\label{tab:minesMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the chain environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & 10618 \\ \hline
$n_s^{-1}$ & 10329 \\ \hline
$n_s^{-2}$ & 7081 \\ \hline
$n_s^{-5}$ & 6630 \\
\hline
\end{tabular}
\label{tab:chainMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the loop environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & 7912 \\ \hline
$n_s^{-1}$ & 7912 \\ \hline
$n_s^{-2}$ & 7912 \\ \hline
$n_s^{-5}$ & 7912 \\
\hline
\end{tabular}
\label{tab:loopMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows different values of the parameter $\epsilon$ for the tic tac toe environment. The result is the average of 10 runs of 200 episodes each.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\epsilon$ & Total reward \\ \hline
$n_s^{-0.5}$ & -5532 \\ \hline
$n_s^{-1}$ & -4269 \\ \hline
$n_s^{-2}$ & -2857 \\ \hline
$n_s^{-5}$ & -2522 \\
\hline
\end{tabular}
\label{tab:tttMean}
\end{center}
\end{table}

\begin{table}[H]
\caption{Shows the results for a 10-armed bandit environment. In the first row the best arm had a 0.8 chance of giving reward 1 and the next best had a chance of 0.5. In the second row the best arm had a 0.8 chance of giving reward 1 and the two next best arms had a chance of 0.7 giving a reward of 1. The expected reward of always choosing the best arm is 16000.}
\begin{center}
\begin{tabular}{|c|c|}
\hline
$\cdot$ & Total reward \\ \hline
$1$ & 15515.8 \\ \hline
$2$ & 15377.5 \\
\hline
\end{tabular}
\label{tab:UCBMean}
\end{center}
\end{table}

\section{Discussion}

Results showed that having a high $\epsilon$ was advantageous in environments like the chain environment while in the mines environment a high $\epsilon$ was causing the agent to step on mines even though it had made that mistake several times before. In the tic tac toe environment it showed to be advantageous to have a low exploration rate. The results supports that the loop environment is highly deterministic and is therefore not considered in this report. Since the results suggested that in most environments it was beneficial to have a low, but still existing, exploration rate. A good trade-off between exploration and exploitation was found when $\epsilon = \frac{1}{n_s}$, where $n_s$ is the number of visits in state $s$. Note that experiments with more episodes than 200 would allow for more exploration than experiments with only 200 episodes. It would therefore be desirable to have an exploration rate that would also depend on the number of episodes. However, the number of episodes are not know prior to the experiment. \newline

There is no data included in this report to support the choice of the learning rate parameter $\alpha$, in the GSVI-algorithm, since there was limited time. The learning rate parameter was set to $1$ since it seemed to be the best choice when running experiments on a few environments. Other candidates tried out where $\alpha = n_{s,a}^{-1}$, $\alpha = n_{s,a}^{-2/3}$ and $\alpha = ln(e + n_{s,a})^{-1}$, where $n_{sa}$ is the number of times action $a$ have been taken in state $s$ before. \newline 

There were no other parameter tried for the UCB algorithm other than the ones stated in the method section. The lecture notes does a great job arguing for why the choice of parameters is good. However, the results support that the algorithm does a good job finding the best arm. \newline



\section{Conclusion}

From the result and discussion it can be concluded that in relatively simple environment and where the experiment runs for a small number of episodes ($\le 200$) it is highly favourable to have a rather small exploration rate. The exploration rate was set to be $\epsilon = n_s^{-1}$ since it seemed to be the best trade-off between exploration and exploitation. Small and non-statistically confirmed test suggested that a learning rate of $1$ was most beneficial.  \newline

Even though the simple action-based aggregation of states initially seemed like a good heuristic, it turned out that the heuristic was worse than choosing random actions on all the tested environments. This might be a consequence of the inability of the algorithms used by the agent to account for change in behavior in the same states. This conclusion was also seen in POMDPs, as the agent would learn something about a state, and later make the wrong decisions because the state had different behavior than earlier observed. Potentially, the action-dependent heuristic for exploration would work better if the agent used an internal model of the MDP while exploring new states instead of relying completely on the observed states, in the same way that this would solve the POMDP problem.

\section{Further remarks}

During the project the mines environment was expanded to be a maze with mines in it and two maxima where one maximum is sub-optimal. This environment was not included in the report since there was simply not time for it. However, the agent was tested on this environment and did find the optimal maximum most of the times with an exploration rate $\epsilon = n_s^{-1}$. \newline 

\end{flushleft}

\end{document}